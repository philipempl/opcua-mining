{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375d12d1",
   "metadata": {},
   "source": [
    "# Process Mining on OPC UA Network Traffic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c924cba",
   "metadata": {},
   "source": [
    "This notebook presents a detailed exploration of process mining techniques applied to OPC UA (Open Platform Communications Unified Architecture) network traffic. OPC UA is a machine-to-machine communication protocol for industrial automation, and this analysis aims to uncover hidden patterns, and process flows within the network traffic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad6ccc",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dependencies and Imports\n",
    "\n",
    "Before we begin the analysis, we need to install and import necessary libraries and modules. The following cells handle this setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pm4py\n",
    "!pip install graphviz\n",
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ebcad",
   "metadata": {},
   "source": [
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6477acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import datetime\n",
    "import psutil\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from pm4py.util import constants\n",
    "from IPython.display import display, Image\n",
    "from pm4py.algo.filtering.dfg import dfg_filtering\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualizer\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "from pm4py.algo.conformance.tokenreplay.diagnostics import root_cause_analysis\n",
    "from pm4py.visualization.decisiontree import visualizer as dt_vis\n",
    "from pm4py.algo.transformation.log_to_features import algorithm as log_to_features\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "from pm4py.algo.conformance import alignments as conformance_alignments\n",
    "from pm4py.algo.discovery.log_skeleton import algorithm as lsk_discovery\n",
    "from pm4py.algo.conformance.log_skeleton import algorithm as lsk_conformance\n",
    "from pm4py.algo.conformance.tokenreplay.diagnostics import duration_diagnostics\n",
    "from pm4py.algo.organizational_mining.sna import util\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "from pm4py.statistics.traces.generic.log import case_arrival\n",
    "from pm4py.algo.organizational_mining.local_diagnostics import algorithm as local_diagnostics\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "from pm4py.algo.discovery.temporal_profile import algorithm as temporal_profile_discovery\n",
    "from pm4py.algo.conformance.temporal_profile import algorithm as temporal_profile_conformance\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pm4py.statistics.traces.generic.log import case_statistics\n",
    "from pm4py.visualization.graphs import visualizer as graphs_visualizer\n",
    "\n",
    "# Define the input file path\n",
    "input_file = './opcua.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c275a",
   "metadata": {},
   "source": [
    "## 2. Understanding the Network Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the provided JSON data\n",
    "with open(input_file, \"r\") as f:\n",
    "    opcua_json = json.load(f)\n",
    "\n",
    "# Extract source and destination IP addresses and ports\n",
    "comm_pairs = []\n",
    "\n",
    "for entry in data:\n",
    "    layers = entry.get('_source', {}).get('layers', {})\n",
    "    \n",
    "    # Extract IP and port information\n",
    "    src_ip = layers.get('ip', {}).get('ip.src')\n",
    "    dst_ip = layers.get('ip', {}).get('ip.dst')\n",
    "    src_port = layers.get('tcp', {}).get('tcp.srcport')\n",
    "    dst_port = layers.get('tcp', {}).get('tcp.dstport')\n",
    "    \n",
    "    if src_ip and dst_ip and src_port and dst_port:\n",
    "        comm_pairs.append((src_ip + ':' + src_port, dst_ip + ':' + dst_port))\n",
    "\n",
    "# Create a new graph for the modified hierarchy\n",
    "G_modified = nx.Graph()\n",
    "\n",
    "# Add edges from port (IP:port) to its associated IP\n",
    "for src, dst in comm_pairs:\n",
    "    src_ip, src_port = src.split(':')\n",
    "    dst_ip, dst_port = dst.split(':')\n",
    "    \n",
    "    # Add edge from port (IP:port) to its IP\n",
    "    G_modified.add_edge(src, src_ip)\n",
    "    G_modified.add_edge(dst, dst_ip)\n",
    "    \n",
    "    # Add edge between source port and destination port\n",
    "    G_modified.add_edge(src, dst)\n",
    "\n",
    "    \n",
    "# Assign colors to nodes\n",
    "modified_node_colors = []\n",
    "for node in G_modified.nodes():\n",
    "    if ':' in node:  # This means it's a port (IP:port combination)\n",
    "        modified_node_colors.append('#b2b2b2')  # Color for ports\n",
    "    else:\n",
    "        modified_node_colors.append('#44AA99')  # Color for IP addresses\n",
    "\n",
    "modified_node_sizes = [2500 if ':' in node else 8000 for node in G_modified.nodes()]\n",
    "\n",
    "# Define labels for the nodes\n",
    "labels = {}\n",
    "for node in G_modified.nodes():\n",
    "    if ':' in node:  # This means it's a port (IP:port combination)\n",
    "        labels[node] = node.split(':')[1]  # Only keep the port number\n",
    "    else:\n",
    "        labels[node] = node  # IP address\n",
    "\n",
    "# Convert the graph to a directed graph\n",
    "G_directed = nx.DiGraph(G_modified)\n",
    "\n",
    "# Calculate the frequency of communications between ports\n",
    "edge_freq = {}\n",
    "for src, dst in comm_pairs:\n",
    "    if (src, dst) in edge_freq:\n",
    "        edge_freq[(src, dst)] += 1\n",
    "    else:\n",
    "        edge_freq[(src, dst)] = 1\n",
    "\n",
    "# Remove directed edges between port (IP:port) and its IP address\n",
    "for src, dst in comm_pairs:\n",
    "    src_ip, src_port = src.split(':')\n",
    "    dst_ip, dst_port = dst.split(':')\n",
    "    \n",
    "    # Remove directed edge from port (IP:port) to its IP\n",
    "    if G_directed.has_edge(src, src_ip):\n",
    "        G_directed.remove_edge(src, src_ip)\n",
    "    if G_directed.has_edge(dst, dst_ip):\n",
    "        G_directed.remove_edge(dst, dst_ip)\n",
    "\n",
    "# Draw the modified directed network graph with frequency edge labels\n",
    "plt.figure(figsize=(19, 8))\n",
    "pos_modified = nx.spring_layout(G_modified, k=0.5, iterations=1000)  # Positioning of nodes\n",
    "nx.draw(\n",
    "    G_directed,\n",
    "    pos_modified,\n",
    "    labels=labels,\n",
    "    node_size=modified_node_sizes,\n",
    "    node_color=modified_node_colors,\n",
    "    font_size=10,\n",
    "    width=0.5,\n",
    "    edge_color=\"black\",\n",
    "    font_color='whitesmoke',\n",
    "    arrowsize=20,\n",
    "    connectionstyle=\"arc3,rad=0.1\"\n",
    ")\n",
    "nx.draw_networkx_edge_labels(G_directed, pos_modified, edge_labels=edge_freq, font_size=9)\n",
    "\n",
    "plt.savefig(\"network_graph.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "num_nodes = G_directed.number_of_nodes()\n",
    "num_edges = G_directed.number_of_edges()\n",
    "density = nx.density(G_directed)\n",
    "degree_centrality = nx.degree_centrality(G_directed)\n",
    "\n",
    "# Identify the top 5 nodes with the highest degree centrality\n",
    "top_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Display the calculated metrics\n",
    "num_nodes, num_edges, density, top_degree_centrality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad6e2e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. OPCUA Packet Analyzer Definition\n",
    "\n",
    "The `OPCUAPacketAnalyzer` class is responsible for analyzing OPC UA packets. It extracts relevant information from the packets and prepares the data for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fe97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPCUAPacketAnalyzer:\n",
    "    def __init__(self, input_file, num_packets):\n",
    "        \"\"\"\n",
    "        Initialize the OPCUAPacketAnalyzer with the input file and the number of packets to analyze.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input JSON file.\n",
    "            num_packets (int): The number of packets to analyze.\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.opc_packets = self.load_data()[:num_packets]\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the JSON file.\n",
    "\n",
    "        Returns:\n",
    "            list: List of JSON data records.\n",
    "        \"\"\"\n",
    "        with open(self.input_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def extract_tcp_data(self, packet):\n",
    "        \"\"\"\n",
    "        Extract TCP data from a packet.\n",
    "\n",
    "        Args:\n",
    "            packet (dict): A packet dictionary.\n",
    "\n",
    "        Returns:\n",
    "            list: List of extracted data.\n",
    "        \"\"\"\n",
    "        return self.extract_data(packet, 'tcp', ['tcp.srcport', 'tcp.dstport'])\n",
    "\n",
    "    def extract_ip_data(self, packet):\n",
    "        \"\"\"\n",
    "        Extract IP data from a packet.\n",
    "\n",
    "        Args:\n",
    "            packet (dict): A packet dictionary.\n",
    "\n",
    "        Returns:\n",
    "            list: List of extracted data.\n",
    "        \"\"\"\n",
    "        return self.extract_data(packet, 'ip', ['ip.src', 'ip.dst'])\n",
    "\n",
    "    def extract_eth_data(self, packet):\n",
    "        \"\"\"\n",
    "        Extract Ethernet data from a packet.\n",
    "\n",
    "        Args:\n",
    "            packet (dict): A packet dictionary.\n",
    "\n",
    "        Returns:\n",
    "            list: List of extracted data.\n",
    "        \"\"\"\n",
    "        return self.extract_data(packet, 'eth', ['eth.src', 'eth.dst'])\n",
    "            \n",
    "    def extract_data(self, packet, layer, fields):\n",
    "        \"\"\"\n",
    "        Extract data from a packet for a specified layer and fields.\n",
    "\n",
    "        Args:\n",
    "            packet (dict): A packet dictionary.\n",
    "            layer (str): The layer to extract data from.\n",
    "            fields (list): List of field names to extract.\n",
    "\n",
    "        Returns:\n",
    "            list: List of extracted data.\n",
    "        \"\"\"\n",
    "        timestamp_str = packet[\"_source\"][\"layers\"][\"frame\"][\"frame.time\"]\n",
    "        timestamp = datetime.datetime.strptime(timestamp_str, \"%b %d, %Y %H:%M:%S.%f000 %Z\")\n",
    "        return [timestamp] + [packet[\"_source\"][\"layers\"][layer].get(field, '') for field in fields]\n",
    "\n",
    "    def write_csv(self, filename, rows):\n",
    "        \"\"\"\n",
    "        Write data to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The name of the CSV file to write.\n",
    "            rows (list): List of rows to write to the CSV file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(rows)\n",
    "\n",
    "    def analyze_packets(self):\n",
    "        \"\"\"\n",
    "        Analyze OPC UA packets and write the results to CSV files.\n",
    "        \"\"\"\n",
    "        matching_ip_ids = self.match_request_handles()\n",
    "        transport_layer_log, network_layer_log, eth_layer_log, app_layer_log = [], [], [], []\n",
    "        transport_layer_log.append([\"time:timestamp\", \"case:concept:name\", \"concept:name\"])\n",
    "        network_layer_log.append([\"time:timestamp\", \"case:concept:name\", \"concept:name\"])\n",
    "        eth_layer_log.append([\"time:timestamp\", \"case:concept:name\", \"concept:name\"])\n",
    "        app_layer_log.append([\"time:timestamp\", \"case:concept:name\", \"org:resource\", \"concept:name\"])\n",
    "\n",
    "        for packet in self.opc_packets:\n",
    "            eth_layer_log.append(self.extract_eth_data(packet))\n",
    "            network_layer_log.append(self.extract_ip_data(packet))\n",
    "            transport_layer_log.append(self.extract_tcp_data(packet))\n",
    "        \n",
    "        matched_array = self.match_request_handles()\n",
    "        tuples = self.add_case_id(matched_array)\n",
    "        app_layer_log = self.transform_tuples(app_layer_log, tuples)\n",
    "        \n",
    "        self.write_csv('link_layer.csv', eth_layer_log)\n",
    "        self.write_csv('network_layer.csv', network_layer_log)\n",
    "        self.write_csv('transport_layer.csv', transport_layer_log)\n",
    "        self.write_csv('app_layer.csv', app_layer_log)\n",
    "\n",
    "    def match_request_handles(self):\n",
    "        \"\"\"\n",
    "        Match request handles in OPC UA packets and create matched arrays.\n",
    "\n",
    "        Returns:\n",
    "            list: List of matched arrays.\n",
    "        \"\"\"\n",
    "        request_array, response_array, matched_array = [], [], []\n",
    "\n",
    "        for packet in self.opc_packets:\n",
    "            if packet[\"_source\"][\"layers\"][\"opcua\"][\"opcua.transport.type\"] == \"MSG\":\n",
    "                encodable_object = packet[\"_source\"][\"layers\"][\"opcua\"][\"OpcUa Service : Encodeable Object\"]\n",
    "                connection_type, header = self.find_connection_type(encodable_object)\n",
    "                if header:\n",
    "                    request_handle = packet[\"_source\"][\"layers\"][\"opcua\"][\"OpcUa Service : Encodeable Object\"][connection_type][f'{header}: {header}'][\"opcua.RequestHandle\"]\n",
    "\n",
    "                    # Extracting additional details\n",
    "                    ip_src = packet[\"_source\"][\"layers\"][\"ip\"][\"ip.src\"]\n",
    "                    ip_dst = packet[\"_source\"][\"layers\"][\"ip\"][\"ip.dst\"]\n",
    "                    tcp_srcport = packet[\"_source\"][\"layers\"][\"tcp\"][\"tcp.srcport\"]\n",
    "                    tcp_dstport = packet[\"_source\"][\"layers\"][\"tcp\"][\"tcp.dstport\"]\n",
    "                    timestamp_str = packet[\"_source\"][\"layers\"][\"frame\"][\"frame.time\"]\n",
    "                    timestamp = datetime.datetime.strptime(timestamp_str, \"%b %d, %Y %H:%M:%S.%f000 %Z\")\n",
    "                    nodes = self.get_nodes(encodable_object, connection_type)\n",
    "                    \n",
    "                    (request_array if header == \"RequestHeader\" else response_array).append([request_handle, ip_src, tcp_srcport, ip_dst, tcp_dstport, timestamp, connection_type, nodes])\n",
    "\n",
    "        matched_array = [(x[5], f\"{x[1]}:{x[2]}\", f\"{x[3]}:{x[4]}\", x[6], [{k: v for k, v in zip(x[7], y[7])}]) for x in request_array for y in response_array if x[0] == y[0]]\n",
    "        matched_array = sorted(matched_array, key=lambda x: x[0])\n",
    "        return matched_array\n",
    "    \n",
    "    def find_connection_type(self, encodable_object):\n",
    "        \"\"\"\n",
    "        Find the connection type and header in the encodable object.\n",
    "\n",
    "        Args:\n",
    "            encodable_object (dict): The encodable object dictionary.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the connection type and header.\n",
    "        \"\"\"\n",
    "        key_list = [\"ReadRequest\", \"ReadResponse\", \"WriteRequest\", \"WriteResponse\", \"SubscribeRequest\", \"SubscribeResponse\", \"PublishRequest\", \"PublishResponse\"]\n",
    "        for key in key_list:\n",
    "            if key in encodable_object.keys():\n",
    "                header = \"RequestHeader\" if 'Request' in key else \"ResponseHeader\"\n",
    "                return key, header\n",
    "        return None, None\n",
    "    \n",
    "    def get_nodes(self, encodable_object, connection_type):\n",
    "        \"\"\"\n",
    "        Extract node information based on the connection type.\n",
    "\n",
    "        Args:\n",
    "            encodable_object (dict): The encodable object dictionary.\n",
    "            connection_type (str): The connection type.\n",
    "\n",
    "        Returns:\n",
    "            list: List of extracted node information.\n",
    "        \"\"\"\n",
    "        nodes = []\n",
    "        if connection_type == \"ReadRequest\" and \"NodesToRead: Array of ReadValueId\" in encodable_object[connection_type]:\n",
    "            array_size = int(encodable_object[connection_type][\"NodesToRead: Array of ReadValueId\"][\"opcua.variant.ArraySize\"])\n",
    "            for i in range(array_size):\n",
    "                obj = encodable_object[connection_type][\"NodesToRead: Array of ReadValueId\"][f\"[{i}]: ReadValueId\"][\"NodeId: NodeId\"]\n",
    "                node = obj[\"opcua.nodeid.string\"] if \"opcua.nodeid.string\" in obj else obj[\"opcua.nodeid.numeric\"]\n",
    "                node = \".\".join(node.split(\".\")[-2:]) if \".\" in node else node\n",
    "                nodes.append(node)\n",
    "        elif connection_type == \"ReadResponse\" and \"Results: Array of DataValue\" in encodable_object[connection_type]:\n",
    "            array_size = int(encodable_object[connection_type][\"Results: Array of DataValue\"][\"opcua.variant.ArraySize\"])\n",
    "            for i in range(array_size):\n",
    "                obj = encodable_object[connection_type][\"Results: Array of DataValue\"][f\"[{i}]: DataValue\"]\n",
    "                key = obj[\"Value: Variant\"][list(obj[\"Value: Variant\"].keys())[1]] if \"Value: Variant\" in obj else None\n",
    "                nodes.append(key)\n",
    "        elif connection_type == \"WriteRequest\" and \"NodesToWrite: Array of WriteValue\" in encodable_object[connection_type]:\n",
    "            array_size = int(encodable_object[connection_type][\"NodesToWrite: Array of WriteValue\"][\"opcua.variant.ArraySize\"])\n",
    "            for i in range(array_size):\n",
    "                obj = encodable_object[connection_type][\"NodesToWrite: Array of WriteValue\"][f\"[{i}]: WriteValue\"][\"NodeId: NodeId\"]\n",
    "                node = obj[\"opcua.nodeid.string\"] if \"opcua.nodeid.string\" in obj else obj[\"opcua.nodeid.numeric\"]\n",
    "                node = \".\".join(node.split(\".\")[-2:]) if \".\" in node else node\n",
    "                nodes.append(node)\n",
    "        elif connection_type == \"WriteResponse\" and \"Results: Array of StatusCode\" in encodable_object[connection_type]:\n",
    "            array_size = int(encodable_object[connection_type][\"Results: Array of StatusCode\"][\"opcua.variant.ArraySize\"])\n",
    "            for _ in range(array_size):\n",
    "                nodes.append(encodable_object[connection_type][\"Results: Array of StatusCode\"][\"opcua.Results\"])\n",
    "        return nodes\n",
    "\n",
    "    def add_case_id(self, matched_array):\n",
    "        \"\"\"\n",
    "        Add case IDs to the matched array.\n",
    "\n",
    "        Args:\n",
    "            matched_array (list): List of matched arrays.\n",
    "\n",
    "        Returns:\n",
    "            list: List of matched arrays with added case IDs.\n",
    "        \"\"\"\n",
    "        ip_to_case_id = {}\n",
    "        new_matched_array = []\n",
    "\n",
    "        for tup in matched_array:\n",
    "            ip = tup[2].split(\":\")[0]\n",
    "            dictionaries = tup[4]\n",
    "\n",
    "            # Check if any dictionary contains a key with \"CAN_PRODUCE_ITEM_ID\"\n",
    "            case_id = None\n",
    "            for dictionary in dictionaries:\n",
    "                if \"CanProduce.CAN_PRODUCE_ITEM_ID\" in dictionary.keys():\n",
    "                    case_id = dictionary.get(\"CanProduce.CAN_PRODUCE_ITEM_ID\")\n",
    "                    ip_to_case_id[ip] = case_id  # Store the case id for this IP\n",
    "                    break\n",
    "\n",
    "            # If we didn't find a \"CAN_PRODUCE_ITEM_ID\", use the previously stored case id for this IP\n",
    "            if case_id is None:\n",
    "                case_id = ip_to_case_id.get(ip, None)\n",
    "\n",
    "            # Create a new tuple with the added case id\n",
    "            new_tup = (*tup, case_id)\n",
    "            new_matched_array.append(new_tup)\n",
    "\n",
    "        return new_matched_array\n",
    "\n",
    "    def transform_tuples(self, app_layer_log, tuples_list):\n",
    "        \"\"\"\n",
    "        Transform tuples and add them to the app layer log.\n",
    "\n",
    "        Args:\n",
    "            app_layer_log (list): List of app layer log entries.\n",
    "            tuples_list (list): List of tuples to transform and add.\n",
    "\n",
    "        Returns:\n",
    "            list: Updated app layer log.\n",
    "        \"\"\"\n",
    "        for tup in tuples_list:\n",
    "            # Extract relevant information\n",
    "            timestamp = tup[0]\n",
    "            case_id = tup[5]\n",
    "            ip_address = tup[2].split(\":\")[0]\n",
    "            request_type = tup[3]\n",
    "            request_keys = [str(k) for k in tup[4][0].keys()]\n",
    "\n",
    "            # Form the new list with the extracted information\n",
    "            new_list = [timestamp, case_id, ip_address, request_type + \" (\" + \", \\n\".join(request_keys) + \")\"]\n",
    "            app_layer_log.append(new_list)\n",
    "\n",
    "        return app_layer_log\n",
    "\n",
    "    def check_format(self, string, pattern):\n",
    "        \"\"\"\n",
    "        Check if a string matches a specified pattern.\n",
    "\n",
    "        Args:\n",
    "            string (str): The string to check.\n",
    "            pattern (str): The pattern to match.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the string matches the pattern, False otherwise.\n",
    "        \"\"\"\n",
    "        return re.match(pattern, string) is not None\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Entry point to run the packet analysis.\n",
    "        \"\"\"\n",
    "        start_time = time.time()  # Start the timer\n",
    "        \n",
    "        self.analyze_packets()\n",
    "        \n",
    "        end_time = time.time()  # End the timer\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        print(f\"Time taken for packet analysis with {len(self.opc_packets)} packets: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"CPU usage: {cpu_usage}%\")\n",
    "        print(f\"RAM usage: {ram_usage}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5148fb0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Running the Packet Analyzer\n",
    "\n",
    "With the `OPCUAPacketAnalyzer` class defined, we can now instantiate it and run the analysis on our input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of OPCUAPacketAnalyzer\n",
    "analyzer = OPCUAPacketAnalyzer(input_file, len(json.load(open(input_file))))\n",
    "\n",
    "# Run the packet analysis\n",
    "analyzer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c1556",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Data Transformation and Process Visualization\n",
    "\n",
    "Once the packets are analyzed, we transform the data into a format suitable for process mining. We then visualize the directly-follows graph for the event log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594af677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the CSV file into a DataFrame\n",
    "dataframe = pd.read_csv(\"app_layer.csv\", sep=\",\")\n",
    "\n",
    "# Format the DataFrame for PM4Py\n",
    "dataframe = pm4py.format_dataframe(dataframe, case_id=\"case:concept:name\", activity_key=\"concept:name\",\n",
    "                                   timestamp_key=\"time:timestamp\")\n",
    "\n",
    "# Convert the formatted DataFrame to an event log\n",
    "event_log = pm4py.convert_to_event_log(dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c3624",
   "metadata": {},
   "source": [
    "### Directly-Follows-Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b37d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the Directly Follows Graph from the event log\n",
    "_df, _sa, _ea = pm4py.discover_directly_follows_graph(event_log)\n",
    "\n",
    "# Save the Directly Follows Graph as an SVG file\n",
    "pm4py.save_vis_dfg(_df, _sa, _ea, 'perf_dfg.svg')\n",
    "\n",
    "# View the Directly Follows Graph as a PNG image\n",
    "pm4py.view_dfg(_df, _sa, _ea, format=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53811fa",
   "metadata": {},
   "source": [
    "### Inductive Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover a Petri net using the inductive miner algorithm with a noise threshold\n",
    "im_net, im_im, im_fm = pm4py.discover_petri_net_inductive(event_log, noise_threshold=0.9)\n",
    "\n",
    "# Create a visualization of the Petri net\n",
    "gviz = pn_visualizer.apply(im_net, im_im, im_fm, log=event_log)\n",
    "\n",
    "# View the Petri net visualization\n",
    "pm4py.visualization.petri_net.visualizer.view(gviz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a7686",
   "metadata": {},
   "source": [
    "### Heuristic Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover a Petri net using the heuristics miner algorithm with a dependency threshold\n",
    "hm_net, hm_im, hm_fm = pm4py.discover_petri_net_heuristics(event_log, dependency_threshold=0.9)\n",
    "\n",
    "# Generate a visualization of the Petri net with frequency-based variant and PNG format\n",
    "viz_variant = pm4py.visualization.petri_net.visualizer.Variants.FREQUENCY\n",
    "viz_parameters = {\"format\": \"png\"}\n",
    "gviz = pm4py.visualization.petri_net.visualizer.apply(hm_net, hm_im, hm_fm, variant=viz_variant, parameters=viz_parameters)\n",
    "\n",
    "# View the Petri net visualization\n",
    "pm4py.visualization.petri_net.visualizer.view(gviz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1997b",
   "metadata": {},
   "source": [
    "### Alpha Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c99c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover a Petri net using the alpha miner algorithm\n",
    "am_net, am_im, am_fm = pm4py.discover_petri_net_alpha(event_log)\n",
    "\n",
    "# Define visualization parameters\n",
    "parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "\n",
    "# Generate the visualization of the Petri net using the alpha miner variant and specified parameters\n",
    "gviz = pn_visualizer.apply(am_net, am_im, am_fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=event_log)\n",
    "\n",
    "# Save the Petri net visualization as a PDF file\n",
    "pn_visualizer.save(gviz, \"alpha-miner.pdf\")\n",
    "\n",
    "# View the Petri net visualization\n",
    "pm4py.visualization.petri_net.visualizer.view(gviz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6b88e",
   "metadata": {},
   "source": [
    "### BPMN 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a143ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover a BPMN model using the inductive miner algorithm with a noise threshold\n",
    "bpmn_model = pm4py.discover_bpmn_inductive(event_log, noise_threshold=0.98)\n",
    "\n",
    "# Write the BPMN model to a BPMN file\n",
    "pm4py.write_bpmn(bpmn_model, \"model.bpmn\")\n",
    "\n",
    "# View the BPMN model\n",
    "pm4py.view_bpmn(bpmn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a0532",
   "metadata": {},
   "source": [
    "### Role Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover organizational roles from the event log\n",
    "roles = pm4py.discover_organizational_roles(event_log)\n",
    "\n",
    "# Print the discovered organizational roles\n",
    "roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98dedb",
   "metadata": {},
   "source": [
    " ## 6. Performance Evaluation\n",
    " \n",
    "We first run the `OPCUAPacketAnalyzer` with varying packet loads to measure the performance in terms of elapsed time, CPU usage, and RAM usage. Then, we pPlot the performance results to visually represent how the system performs as the packet load increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbb449",
   "metadata": {},
   "source": [
    "### Run OPCUAPacketAnalyzer with different packet loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store performance results\n",
    "performance_results = []\n",
    "\n",
    "# Get the total number of packets from the input file\n",
    "total_packets = len(json.load(open(input_file)))\n",
    "\n",
    "# Define the increment for the number of packets\n",
    "increment = 1000\n",
    "\n",
    "# Iterate over packet counts from 'increment' to 'total_packets' with steps of 'increment'\n",
    "for i in range(increment, total_packets + increment, increment):\n",
    "    # Create an instance of OPCUAPacketAnalyzer with a specific number of packets\n",
    "    analyzer = OPCUAPacketAnalyzer(input_file, num_packets=i)\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the packet analysis\n",
    "    analyzer.run()\n",
    "    \n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Measure CPU and RAM usage\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    ram_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Append performance metrics to the results list\n",
    "    performance_results.append([i, elapsed_time, cpu_usage, ram_usage])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e60e5b",
   "metadata": {},
   "source": [
    "### Plot performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from performance_results\n",
    "df = pd.DataFrame(performance_results, columns=['Packets', 'Time', 'CPU Usage', 'RAM Usage'])\n",
    "\n",
    "# Plotting Time\n",
    "packets_array = df['Packets'].values\n",
    "time_taken = df['Time'].values\n",
    "\n",
    "# Fit the polynomial regression again\n",
    "coeffs_time = np.polyfit(packets_array, time_taken, 2)\n",
    "p_time = Polynomial(coeffs_time[::-1])\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax1 = plt.figure(figsize=(16, 8)), plt.gca()\n",
    "\n",
    "# Time vs Packets plot\n",
    "ax1.scatter(df['Packets'], df['Time'], color='black', label='Actual Time Data', s=60)\n",
    "ax1.plot(df['Packets'], p_time(packets_array), color='black', label='Polynomial Regression of Time', linewidth=2)\n",
    "ax1.set_xlabel('Number of Packets', fontsize=14, weight='bold')\n",
    "ax1.set_ylabel('Time Taken (seconds)', fontsize=14, weight='bold')\n",
    "ax1.tick_params(axis='y', labelsize=12)\n",
    "ax1.tick_params(axis='x', labelsize=12)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Create a second y-axis for CPU and RAM Usage\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df['Packets'], df['CPU Usage'], 'g-.', marker='o', label='CPU Usage (%)', linewidth=2, markersize=8)\n",
    "ax2.plot(df['Packets'], df['RAM Usage'], 'm-.', marker='x', label='RAM Usage (%)', color='purple', linewidth=2, markersize=8)\n",
    "ax2.set_ylabel('Usage (%)', fontsize=14, weight='bold')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# Adjusting legend to fit all descriptions\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=4, frameon=False, fontsize=14)\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "fig.savefig(\"opcua_performance_analysis.pdf\", bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbdaf6",
   "metadata": {},
   "source": [
    "### Print the polynomial regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30859064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor and pretty print polynomial coefficients\n",
    "for i, coeff in enumerate(coeffs_time):\n",
    "    print(f\"coeffs_time[{i}]: {coeff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecb2c2",
   "metadata": {},
   "source": [
    "### Exemplify the duration for 1,000,000 OPC UA packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0317105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_formula(x, coeffs):\n",
    "    \"\"\"\n",
    "    Given x (number of packets), compute y (time) using the polynomial regression formula.\n",
    "\n",
    "    Parameters:\n",
    "        x (float): The number of packets.\n",
    "        coeffs (tuple): Coefficients (a, b, c) of the polynomial regression model.\n",
    "\n",
    "    Returns:\n",
    "        float: Predicted time.\n",
    "    \"\"\"\n",
    "    a, b, c = coeffs\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "# Example usage: Predict the time required for 1,000,000 packets\n",
    "predicted_time = polynomial_regression_formula(1000000, coeffs_time)\n",
    "print(f\"Predicted time for 1,000,000 packets: {predicted_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f269a2",
   "metadata": {},
   "source": [
    "## 7. Quality Evaluation\n",
    "\n",
    "Next, we define and use functions to evaluate and visualize the quality of a Petri net. The `evaluate_petri_net`function assesses the Petri net's fitness, precision, generalization, and simplicity using various mining techniques (inductive, heuristic, or alpha) and thresholds. Last, we visualize these results with line charts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaed9c8",
   "metadata": {},
   "source": [
    "### Define quality functions for different process discovery methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4020df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_petri_net(miner, log, threshold):\n",
    "    \"\"\"\n",
    "    Evaluate the quality criteria of a Petri net using specified mining algorithm and threshold.\n",
    "\n",
    "    Parameters:\n",
    "        miner (str): Mining algorithm, one of \"inductive\", \"heuristic\", or \"alpha\".\n",
    "        log (event_log): Event log to be used for evaluation.\n",
    "        threshold (float): Threshold for mining algorithm (dependency_threshold for \"inductive\",\n",
    "                           noise_threshold for \"heuristic\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if miner == \"inductive\":\n",
    "        net, im, fm = pm4py.discover_petri_net_heuristics(log, dependency_threshold=threshold)\n",
    "    elif miner == \"heuristic\":\n",
    "        net, im, fm = pm4py.discover_petri_net_inductive(log, noise_threshold=threshold)\n",
    "    else:\n",
    "        net, im, fm = pm4py.discover_petri_net_alpha(log)\n",
    "\n",
    "    # Calculate fitness using token-based replay\n",
    "    tbr_fitness = pm4py.fitness_token_based_replay(log, net, im, fm)\n",
    "    fitness = round(tbr_fitness['perc_fit_traces'], 4)\n",
    "\n",
    "    # Calculate precision using token-based replay\n",
    "    etc_prec = pm4py.precision_token_based_replay(log, net, im, fm)\n",
    "    precision = round(etc_prec * 100, 4)\n",
    "\n",
    "    # Calculate generalization\n",
    "    gen = generalization_evaluator.apply(log, net, im, fm)\n",
    "    generalization = round(gen * 100, 4)\n",
    "\n",
    "    # Calculate simplicity\n",
    "    simp = simplicity_evaluator.apply(net)\n",
    "    simplicity = round(simp * 100, 4)\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(f\"Fitness: {fitness}%\")\n",
    "    print(f\"Precision: {precision}%\")\n",
    "    print(f\"Generalization: {generalization}%\")\n",
    "    print(f\"Simplicity: {simplicity}%\")\n",
    "\n",
    "    # Store the evaluation results in the data list\n",
    "    data.append((miner, threshold, fitness, precision, generalization, simplicity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489517c",
   "metadata": {},
   "source": [
    "### Run the quality assessment for the inductive and heuristic miners with different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "miners = [\"inductive\", \"heuristic\"]\n",
    "\n",
    "# Evaluate Petri nets for different mining algorithms and thresholds\n",
    "for miner in miners:\n",
    "    for threshold in thresholds:\n",
    "        print(f\"{miner}: {threshold}\")\n",
    "        evaluate_petri_net(miner, event_log, threshold)\n",
    "\n",
    "# Append evaluation results for the \"alpha\" miner with fixed values\n",
    "for threshold in thresholds:\n",
    "    data.append((\"alpha\", threshold, 0.0, 19.438, 87.0583, 77.777))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8ff94",
   "metadata": {},
   "source": [
    "### Visualize the results for each miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pm_quality_plot(data, name):\n",
    "    \"\"\"\n",
    "    Create a quality plot for Petri nets based on evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): List of tuples containing evaluation data for different thresholds.\n",
    "        name (str): Name of the output plot file (e.g., \"quality_plot\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Extracting metrics for plotting\n",
    "    thresholds = [entry[0] for entry in data]\n",
    "    fitness = [entry[1] for entry in data]\n",
    "    precision = [entry[2] for entry in data]\n",
    "    generalization = [entry[3] for entry in data]\n",
    "    simplicity = [entry[4] for entry in data]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Miner plot\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.plot(thresholds, fitness, marker='o', label='Fitness', color='#44AA99', linewidth=5)\n",
    "    ax.plot(thresholds, precision, marker='o', label='Precision', color='#DDCC77', linewidth=5)\n",
    "    ax.plot(thresholds, generalization, marker='o', label='Generalization', color='#88CCEE', linewidth=5)\n",
    "    ax.plot(thresholds, simplicity, marker='o', label='Simplicity', color='#CC6677', linewidth=5)\n",
    "    ax.set_xlabel('Dependency Threshold', fontsize=17, weight='bold')\n",
    "    ax.set_ylabel('Evaluation Metric (%)', fontsize=17, weight='bold')\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False, fontsize=17)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into different categories based on mining algorithms\n",
    "inductive_data = [entry[1:] for entry in data if entry[0] == 'inductive']\n",
    "heuristic_data = [entry[1:] for entry in data if entry[0] == 'heuristic']\n",
    "alpha_data = [entry[1:] for entry in data if entry[0] == 'alpha']\n",
    "\n",
    "# Generate quality plots for different mining algorithms\n",
    "pm_quality_plot(inductive_data, \"inductive\")\n",
    "pm_quality_plot(heuristic_data, \"heuristic\")\n",
    "pm_quality_plot(alpha_data, \"alpha\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
